# -*- coding: utf-8 -*-
"""Rnn_mnist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DpGjDhOGsNQwX_tBa4XQ1F8rRYTL-mKZ
"""

# Cell 1: Setup & Imports
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

print("TensorFlow:", tf.__version__)
tf.random.set_seed(42)
np.random.seed(42)

# Cell 2: Explain & Load MNIST
# MNIST = 70,000 images of digits (0..9), each 28x28 grayscale
# Standard split: 60,000 train / 10,000 test (from tf.keras.datasets)
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

print("Train images:", x_train.shape, "Train labels:", y_train.shape)
print("Test images:", x_test.shape,  "Test labels:", y_test.shape)

# Show a few examples
plt.figure(figsize=(8, 2))
for i in range(8):
    plt.subplot(1, 8, i+1)
    plt.imshow(x_train[i], cmap="gray")
    plt.title(int(y_train[i]))
    plt.axis("off")
plt.suptitle("MNIST samples (28x28)")
plt.show()

# Cell 3: Preprocess
# 1) Normalize pixel values to [0,1]
x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0

# 2) Treat each image row as one time step:
#    (samples, time_steps, features) = (N, 28, 28)
x_train = x_train.reshape((-1, 28, 28))
x_test  = x_test.reshape((-1, 28, 28))

# Build tf.data pipelines for speed
BATCH_SIZE = 128
AUTOTUNE = tf.data.AUTOTUNE

train_ds = (
    tf.data.Dataset.from_tensor_slices((x_train, y_train))
    .shuffle(10000)
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
)

test_ds = (
    tf.data.Dataset.from_tensor_slices((x_test, y_test))
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
)

print("Train ds:", train_ds)
print("Test ds:", test_ds)

# Cell 4: Build a simple RNN classifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

model = Sequential([
    # Input: sequences length 28, each step has 28 features
    SimpleRNN(128, activation="tanh", input_shape=(28, 28)),
    Dense(10, activation="softmax")
])

model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

# Cell 5: Train for 6 epochs
EPOCHS = 6
history = model.fit(
    train_ds,
    validation_data=test_ds,
    epochs=EPOCHS,
    verbose=1
)

# Cell 6: Plot training curves
plt.figure()
plt.plot(history.history["loss"], label="train loss")
plt.plot(history.history["val_loss"], label="val loss")
plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Loss over Epochs")
plt.legend(); plt.show()

plt.figure()
plt.plot(history.history["accuracy"], label="train acc")
plt.plot(history.history["val_accuracy"], label="val acc")
plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.title("Accuracy over Epochs")
plt.legend(); plt.show()

# Cell 7: Evaluate
test_loss, test_acc = model.evaluate(test_ds, verbose=0)
print(f"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}")

# Cell 8: Inference â€” predict on a few test images and visualize
num_samples = 6
idx = np.random.choice(len(x_test), num_samples, replace=False)
sample_imgs = x_test[idx]           # shape: (num_samples, 28, 28)
sample_labels = y_test[idx]

probs = model.predict(sample_imgs, verbose=0)  # (num_samples, 10)
preds = np.argmax(probs, axis=1)

for i in range(num_samples):
    plt.figure()
    plt.imshow(sample_imgs[i], cmap="gray")
    plt.title(f"True: {sample_labels[i]} | Pred: {preds[i]}")
    plt.axis("off")
    plt.show()

print("Predicted class probabilities for the first sample:\n", probs[0])

"""# New Section"""